{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MLOps Assignment 1: Differential Privacy Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "IpgB2Rs4V_oP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Eb4r0ejUWBfP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Nxl0un2XWDfo"
      },
      "outputs": [],
      "source": [
        "import tensorflow_privacy\n",
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer_keras import DPKerasSGDOptimizer\n",
        "from tensorflow_privacy.privacy.analysis.compute_dp_sgd_privacy_lib import compute_dp_sgd_privacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Data and Prepare it for Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2-HmdI0IYOKW"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(\"/content/athletes_v2_train.csv\")\n",
        "test_df = pd.read_csv(\"/content/athletes_v2_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QuZLpzYZYbhW"
      },
      "outputs": [],
      "source": [
        "# structure train and test X and Y dfs\n",
        "features = ['age', 'weight', 'height', 'gender']\n",
        "target_column = 'total_lift'\n",
        "\n",
        "x_train = train_df[features]\n",
        "y_train = train_df[target_column].values\n",
        "\n",
        "x_test = test_df[features]\n",
        "y_test = test_df[target_column].values\n",
        "\n",
        "# one-hot encoding for categorical col gender\n",
        "x_train = pd.get_dummies(x_train, columns=['gender'])\n",
        "x_test = pd.get_dummies(x_test, columns=['gender'])\n",
        "\n",
        "# reindex\n",
        "x_test = x_test.reindex(columns=x_train.columns, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61eCt93wYoxz"
      },
      "outputs": [],
      "source": [
        "# normalize numerical columns \n",
        "numerical_cols = ['age', 'weight', 'height']\n",
        "scaler = StandardScaler()\n",
        "\n",
        "x_train[numerical_cols] = scaler.fit_transform(x_train[numerical_cols])\n",
        "x_test[numerical_cols] = scaler.transform(x_test[numerical_cols])\n",
        "\n",
        "# need to convert to numpy arrays for the model to work\n",
        "x_train = x_train.values.astype(np.float32)\n",
        "x_test = x_test.values.astype(np.float32)\n",
        "y_train = y_train.astype(np.float32)\n",
        "y_test = y_test.astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DP Model Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrzqncDDY9C3"
      },
      "outputs": [],
      "source": [
        "# DP training params\n",
        "learning_rate = 0.15\n",
        "noise_multiplier = 1.1\n",
        "l2_norm_clip = 1.0\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "num_microbatches = batch_size  # note: num_microbatches must be same as batch size \n",
        "\n",
        "# create tf dfs for model training and use batch size consistency to avoid errors \n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=len(x_train))\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# define model \n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(x_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "   tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "\n",
        "# dp model example uses SGD optimizer \n",
        "optimizer = DPKerasSGDOptimizer(\n",
        "    l2_norm_clip=l2_norm_clip,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    num_microbatches=num_microbatches,\n",
        "    learning_rate=learning_rate\n",
        ")\n",
        "\n",
        "# per-example loss (no reduction) to avoid errors \n",
        "loss = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['mae', 'mse'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnARx_8YZJPN",
        "outputId": "66a2d092-7e7d-44a2-8466-c7ebb005b809"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "375/375 [==============================] - 5s 9ms/step - loss: 609985.2500 - mae: 687.2795 - mse: 609985.2500 - val_loss: 46352.6406 - val_mae: 169.1824 - val_mse: 46352.6406\n",
            "Epoch 2/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 32844.1719 - mae: 141.6830 - mse: 32844.1719 - val_loss: 32008.9004 - val_mae: 139.0945 - val_mse: 32008.9004\n",
            "Epoch 3/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 30813.2109 - mae: 136.8077 - mse: 30813.2109 - val_loss: 31855.3223 - val_mae: 138.2531 - val_mse: 31855.3223\n",
            "Epoch 4/10\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 30687.7871 - mae: 136.2754 - mse: 30687.7871 - val_loss: 31957.5938 - val_mae: 138.1384 - val_mse: 31957.5938\n",
            "Epoch 5/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 30777.7422 - mae: 136.1859 - mse: 30777.7422 - val_loss: 31966.7559 - val_mae: 138.0439 - val_mse: 31966.7559\n",
            "Epoch 6/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 30798.7500 - mae: 136.1651 - mse: 30798.7500 - val_loss: 32149.6738 - val_mae: 138.1769 - val_mse: 32149.6738\n",
            "Epoch 7/10\n",
            "375/375 [==============================] - 4s 10ms/step - loss: 30906.4473 - mae: 136.2369 - mse: 30906.4473 - val_loss: 32066.8379 - val_mae: 138.1140 - val_mse: 32066.8379\n",
            "Epoch 8/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 30902.2500 - mae: 136.2437 - mse: 30902.2500 - val_loss: 32076.4707 - val_mae: 138.1149 - val_mse: 32076.4707\n",
            "Epoch 9/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 30877.2871 - mae: 136.2300 - mse: 30877.2871 - val_loss: 32120.4238 - val_mae: 138.1262 - val_mse: 32120.4238\n",
            "Epoch 10/10\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 30879.9980 - mae: 136.1824 - mse: 30879.9980 - val_loss: 32113.1992 - val_mae: 138.1129 - val_mse: 32113.1992\n"
          ]
        }
      ],
      "source": [
        "# train model \n",
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs = epochs,\n",
        "    validation_data = test_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fdh60Cj_a5LD",
        "outputId": "036c3bbb-cb60-4383-cd76-312d0913f625"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "188/188 [==============================] - 0s 1ms/step\n",
            "Test RMSE: 179.0622\n"
          ]
        }
      ],
      "source": [
        "# predict on test set and calculate RMSE \n",
        "predictions = model.predict(x_test).flatten()\n",
        "\n",
        "rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n",
        "print(f\"Test RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpLXD7ePbKc5",
        "outputId": "54de6e1b-b051-4d70-ffaa-cdd58792452e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test R^2: 0.5846\n"
          ]
        }
      ],
      "source": [
        "# calculate R^2\n",
        "ss_res = np.sum((y_test - predictions) ** 2)  \n",
        "ss_tot = np.sum((y_test - np.mean(y_test)) ** 2)  \n",
        "r2 = 1 - (ss_res / ss_tot)\n",
        "\n",
        "print(f\"Test R^2: {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The DP model performs very similar to the non-DP model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxXXsnQnbdc6",
        "outputId": "bf414130-374e-46ee-fafb-61a407983a60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`compute_dp_sgd_privacy` is deprecated. It does not account for doubling of sensitivity with microbatching, and assumes Poisson subsampling, which is rarely used in practice. Please use `compute_dp_sgd_privacy_statement`, which provides appropriate context for the guarantee. To compute epsilon under different assumptions than those in `compute_dp_sgd_privacy_statement`, call the `dp_accounting` libraries directly.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DP epsilon after training: 0.784 for delta=4.164584374479427e-05\n"
          ]
        }
      ],
      "source": [
        "# calculate DP epsilon \n",
        "num_train_examples = len(x_train)\n",
        "batch_size = 64\n",
        "noise_multiplier = 1.1\n",
        "epochs = 10\n",
        "delta = 1 / num_train_examples \n",
        "\n",
        "epsilon, _ = compute_dp_sgd_privacy(\n",
        "    n=num_train_examples,\n",
        "    batch_size=batch_size,\n",
        "    noise_multiplier=noise_multiplier,\n",
        "    epochs=epochs,\n",
        "    delta=delta\n",
        ")\n",
        "\n",
        "print(f\"DP epsilon after training: {epsilon:.3f} for delta={delta}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The DP epsilon is 0.784, indicating a moderate privacy level used in the model. There is room for potential improvement to lower the epsilon value and increase the privacy level."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
